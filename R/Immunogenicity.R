#' Immunogenicity prediction.
#'
#' \code{Immunogenicity} is a wrapper function for training and evaluating an extraTrees classifier.\cr
#' \code{Immunogenicity_Benchmark} is a wrapper function for benchmarking classifiers implemented in the \code{mlr} package.\cr
#'
#' @param preprocessedDFList An output generated by \code{Features_Preprocess}, \code{Features_CorFilter}, or \code{Features_FeatureSelect}.
#' @param featureSet A set of features to be used for model training. If "all", all features in the training dataframe except "DataType", "Immunogenicity", "Peptide", and "Cluster" will be used.
#' @param learnerSet (Optional) A set of learner strings to be used for model benchmarking. Set \code{NULL} to ignore.
#' @param omitLearnerSet (Optional) A set of learner strings to be omitted from model benchmarking. Set \code{NULL} to ignore.
#' @param destDir A directory for loading & exporting data.
#' @param outputHeader A file/folder name header.
#' @param maxJavaMemory The upper limit of memory for Java virtual machine in megabytes.
#' @param coreN The number of cores to be used for parallelization. Set \code{NULL} to skip parallelization.
#' @importFrom dplyr %>%
#' @importFrom dplyr select
#' @importFrom dplyr filter
#' @importFrom dplyr slice
#' @importFrom dplyr mutate
#' @importFrom dplyr bind_cols
#' @importFrom dplyr bind_rows
#' @importFrom dplyr left_join
#' @importFrom dplyr group_by
#' @importFrom dplyr summarise
#' @importFrom readr read_csv
#' @importFrom readr write_csv
#' @importFrom stringr str_split
#' @importFrom stringr fixed
#' @importFrom pacman p_install
#' @importFrom pacman p_unload
#' @importFrom DescTools Sort
#' @importFrom data.table as.data.table
#' @importFrom data.table rbindlist
#' @importFrom parallel detectCores
#' @importFrom pbapply pblapply
#' @importFrom extraTrees extraTrees
#' @importFrom extraTrees setJavaMemory
#' @importFrom caret confusionMatrix
#' @importFrom pROC roc
#' @importFrom pROC auc
#' @importFrom mlr makeClassifTask
#' @importFrom mlr getTaskTargets
#' @importFrom mlr listLearners
#' @importFrom mlr timeboth
#' @importFrom mlr setAggregation
#' @importFrom mlr train.mean
#' @importFrom mlr auc
#' @importFrom mlr logloss
#' @importFrom mlr acc
#' @importFrom mlr kappa
#' @importFrom mlr ber
#' @importFrom mlr benchmark
#' @importFrom mlr makeLearner
#' @importFrom mlr makeResampleDesc
#' @importFrom parallelMap parallelStartSocket
#' @importFrom parallelMap parallelStop
#' @export
#' @rdname Immunogenicity
#' @name Immunogenicity
Immunogenicity <- function(
  preprocessedDFList=NULL, featureSet="all",
  destDir="./Immunogenicity/", outputHeader="Output",
  maxJavaMemory=6000, coreN=parallel::detectCores()
){
  # Working environment
  dir.create(destDir, showWarnings=F, recursive=T)
  extraTrees::setJavaMemory(maxJavaMemory)

  # Internally used functions
  machine_learning <- function(preprocessedDFList, i){
    ## Parameter check
    param <- names(preprocessedDFList)[i]
    s <- try(as.integer(rev(unlist(stringr::str_split(param, stringr::fixed("."))))[1]), silent=T)
    if(any(class(s)=="try-error")) s <- 123456789  ## ad hoc seed
    set.seed(s)

    ## Skipping
    out <- paste0(destDir, "/", outputHeader, "_", param, "_")
    skipQ <- file.exists(paste0(out, "Predictions.csv")) & file.exists(paste0(out, "PerformanceMeasures.csv"))
    if(skipQ){
      cat("Machine learning was skipped.\n")
      return(NULL)
    }

    ## Input data check
    dt <- preprocessedDFList[[i]]$"dt"
    if(is.null(dt$"DataType")){
      message("The input datatable is not valid! 'DataType' column is required.")
      return(NULL)
    }
    if(identical(featureSet, "all")) featureSet <- colnames(dt)
    featureSet <- unique(c("Immunogenicity", setdiff(featureSet, c("DataType", "Peptide", "Cluster"))))
    dt_train <- dt %>%
      dplyr::filter(DataType=="Train") %>%
      dplyr::select(featureSet) %>%
      as.data.frame()
    dt_test <- dt %>%
      dplyr::filter(DataType=="Test") %>%
      dplyr::select(featureSet) %>%
      as.data.frame()

    ## Model training
    cat("Constructing model...\n")
    extraTrees_train_wrapper <- function(orig, featSet=featureSet){
      trgt <- orig$"Immunogenicity"
      tab <- as.numeric(table(trgt))
      w <- 1/tab[trgt]
      dat <- as.matrix(dplyr::select(orig, setdiff(featSet, "Immunogenicity")))
      mod <- extraTrees::extraTrees(
        x=dat, y=orig$"Immunogenicity",
        mtry=35, numRandomCuts=2, weights=w,
        numThreads=ifelse(is.null(coreN), 1, coreN)
      )
      return(mod)
    }
    mod <- extraTrees_train_wrapper(dt_train)

    ## Prediction
    cat("Predicting immunogenicity...\n")
    extraTrees_predict_wrapper <- function(et, newdata, featSet=featureSet){
      dat <- as.matrix(dplyr::select(newdata, setdiff(featSet, "Immunogenicity")))
      pred <- cbind(
        data.frame("PredictedImmunogenicity"=predict(et, dat, probability=F)),
        as.data.frame(predict(et, dat, probability=T))
      )
      return(pred)
    }
    extraTrees_performance_wrapper <- function(pred, orig){
      measure.auc <- as.numeric(pROC::auc(pROC::roc(orig$"Immunogenicity", pred$"Positive")))
      measure.cm <- caret::confusionMatrix(pred$"PredictedImmunogenicity", orig$"Immunogenicity")
      measures <- c(measure.auc, measure.cm$overall[c("Accuracy", "Kappa")], measure.cm$byClass)
      names(measures) <- c("AUC", "Accuracy", "Kappa", "Sensitivity", "Specificity", "PPV", "NPV", "Precision", "Recall", "F1", "Prevalence", "DetectionRate", "DetectionPrevalence", "BalancedAccuracy")
      measures <- as.list(measures)
      return(measures)
    }
    pred_train <- extraTrees_predict_wrapper(mod, newdata=dt_train)
    pred_test <- extraTrees_predict_wrapper(mod, newdata=dt_test)
    measureDF <- data.table::rbindlist(list(
      data.frame("Param"=param, "DataType"="Train", as.data.frame(extraTrees_performance_wrapper(pred_train, dt_train))),
      data.frame("Param"=param, "DataType"="Test", as.data.frame(extraTrees_performance_wrapper(pred_test, dt_test)))
    ))
    predDF <- data.table::rbindlist(list(
      data.frame("Param"=param, "DataType"="Train", as.data.frame(pred_train)),
      data.frame("Param"=param, "DataType"="Test", as.data.frame(pred_test))
    ))
    print(measureDF)
    readr::write_csv(predDF, paste0(out, "Predictions.csv"))
    readr::write_csv(measureDF, paste0(out, "PerformanceMeasures.csv"))

    ## Closing
    rm(list=ls())
    gc();gc()
    return(NULL)
  }
  concatenate_results <- function(destDir){
    cat("Concatenating results...\n")
    predDFList <- pbapply::pblapply(list.files(pattern="Predictions.csv$", path=destDir, full.names=T), function(f){suppressMessages(readr::read_csv(f, col_types='cc_dd'))})
    measureDFList <- pbapply::pblapply(list.files(pattern="PerformanceMeasures.csv$", path=destDir, full.names=T), function(f){suppressMessages(readr::read_csv(f))})
    predDF <- data.table::rbindlist(predDFList)
    measureDF <- data.table::rbindlist(measureDFList)
    readr::write_csv(predDF, file.path(destDir, paste0(outputHeader, "_Predictions_Combined.csv")))
    readr::write_csv(measureDF, file.path(destDir, paste0(outputHeader, "_PerformanceMeasures_Combined.csv")))
    return(list("predDF"=predDF, "measureDF"=measureDF))
  }

  # Main analysis workflow
  if(!is.null(preprocessedDFList)){
    # Input formatting
    if(any(sapply(preprocessedDFList, is.data.frame))){
      preprocessedDFList <- lapply(preprocessedDFList, function(d){list("dt"=d)})
    }
    # Batch machine learning
    leng <- length(preprocessedDFList)
    cat("Number of datasets = ", leng, "\n", sep="")
    res <- lapply(i:leng, function(i){
      cat(i, "/", leng, "| Parameter set: ", param, "\n", sep="")
      try(machine_learning(preprocessedDFList, i), silent=T)
    })
  }

  # Outputs
  res <- concatenate_results(destDir)
  rm(list=setdiff(ls(), c("res")))
  gc();gc()
  return(res)
}

#' @export
#' @rdname Immunogenicity
#' @name Immunogenicity_Benchmark
Immunogenicity_Benchmark <- function(
  preprocessedDFList, learnerSet=NULL, omitLearnerSet=NULL,
  destDir="./Benchmark/", maxJavaMemory=6000, coreN=parallel::detectCores()
){
  # Working environment
  dir.create(destDir, showWarnings=F, recursive=T)
  extraTrees::setJavaMemory(maxJavaMemory)

  # Input check
  if(any(class(preprocessedDFList[[1]])=="data.frame")) preprocessedDFList <- lapply(preprocessedDFList, function(d){list("dt"=d)})

  # Tasks
  preprocessedDFList_to_taskList <- function(preprocessedDFList, maxRowN=Inf){
    pbapply::pblapply(1:length(preprocessedDFList), function(i){
      param <- names(preprocessedDFList)[i]
      s <- try(as.integer(rev(unlist(stringr::str_split(param, stringr::fixed("."))))[1]), silent=T)
      if(any(class(s)=="try-error")) s <- 123456789  ## ad hoc seed
      set.seed(s)
      dt <- preprocessedDFList[[i]][["dt"]] %>% dplyr::select(-DataType, -Peptide, -Cluster) %>% as.data.frame()
      if(nrow(dt)>maxRowN) dt <- dplyr::slice(dt, sample(1:nrow(dt), maxRowN))
      tsk <- mlr::makeClassifTask(id=paste0("Benchmark_", param), data=dt, target="Immunogenicity")
      trgt <- mlr::getTaskTargets(tsk)
      tab <- as.numeric(table(trgt))
      w <- 1/tab[trgt]
      tsk <- mlr::makeClassifTask(id=paste0("Benchmark_", param), data=dt, target="Immunogenicity", weights=w)
      return(tsk)
    })
  }
  message("Converting preprocessed dataframes into a list of tasks...")
  taskSet <- preprocessedDFList_to_taskList(preprocessedDFList, maxRowN=Inf)
  gc();gc()

  # Learners
  message("Preparing required packages...")
  lrns.df <- suppressWarnings(mlr::listLearners("classif", properties=c("twoclass", "prob")))
  if(is.null(learnerSet)) learnerSet <- lrns.df$"class"
  if(is.null(omitLearnerSet)) omitLearnerSet <- c()
  omitLearnerSet <- c(omitLearnerSet, gsub(".rds", "", gsub("BenchmarkResult_", "", basename(list.files(pattern="^BenchmarkResult.+rds$", path=destDir, full.names=T))), fixed=T))
  learnerSet <- setdiff(learnerSet, omitLearnerSet)
  lrns.df <- dplyr::filter(lrns.df, class %in% learnerSet)
  lrns.pkg <- lrns.df$package
  lrns.pkg <- unique(unlist(stringr::str_split(lrns.pkg, stringr::fixed(","))))
  lrns.pkg <- setdiff(lrns.pkg, as.character(as.data.frame(installed.packages())$"Package"))
  if(length(lrns.pkg)>=1) pacman::p_install(lrns.pkg, character.only=T)
  gc();gc()

  # Benchmarking
  message("Starting benchmark experiments...")
  bm_wrapper <- function(learnerString, tasks){
    require("mlr")
    require("parallelMap")
    set.seed(12345)
    message(learnerString)
    msrs <- list(
      mlr::timeboth, mlr::setAggregation(mlr::timeboth, mlr::train.mean),
      mlr::auc, mlr::setAggregation(mlr::auc, mlr::train.mean),
      mlr::logloss, mlr::setAggregation(mlr::logloss, mlr::train.mean),
      mlr::acc, mlr::setAggregation(mlr::acc, mlr::train.mean),
      mlr::kappa, mlr::setAggregation(mlr::kappa, mlr::train.mean),
      mlr::ber, mlr::setAggregation(mlr::ber, mlr::train.mean)
    )
    if(is.null(coreN)) coreN <- 1
    parallelMap::parallelStartSocket(cpus=coreN)
    bmr <- mlr::benchmark(
      learners=mlr::makeLearner(learnerString, predict.type="prob"),
      tasks=tasks,
      measures=msrs,
      resamplings=mlr::makeResampleDesc(method="CV", iters=3, predict="both"),
      show.info=T
    )
    parallelMap::parallelStop()
    print(bmr)
    saveRDS(bmr, file.path(destDir, paste0("BenchmarkResult_", learnerString, ".rds")))
    bmr.df <- as.data.frame(bmr)
    readr::write_csv(bmr.df, file.path(destDir, paste0("BenchmarkResult_", learnerString, ".csv")))
    try(pacman::p_unload("all"), silent=T)
    rm(list=ls())
    gc();gc()
  }
  for(l in learnerSet){ try(bm_wrapper(l, taskSet)) }
  message("Integrating benchmark results...")
  bmfiles <- list.files(pattern="^BenchmarkResult.+csv$", path=destDir, full.names=T)
  bmfiles <- setdiff(bmfiles, file.path(destDir, "BenchmarkResult_Combined.csv"))
  bmr <- dplyr::bind_rows(lapply(bmfiles, readr::read_csv))
  bmr.ord <- bmr %>%
    dplyr::group_by(learner.id) %>% dplyr::summarise(auc=mean(auc)) %>%
    DescTools::Sort(ord="auc", decreasing=F) %>%
    dplyr::select(-auc)
  bmr <- dplyr::left_join(bmr.ord, bmr)
  readr::write_csv(bmr, file.path(destDir, "BenchmarkResult_Combined.csv"))
  return(bmr)
}

